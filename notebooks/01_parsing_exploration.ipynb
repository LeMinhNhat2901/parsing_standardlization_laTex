{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417a55fe",
   "metadata": {},
   "source": [
    "# ğŸ“‚ 01. LaTeX Parsing Exploration\n",
    "\n",
    "**Lab 02: Parsing & Reference Matching**  \n",
    "**MSSV:** 23120067 - LÃª Minh Nháº­t  \n",
    "**Má»¥c tiÃªu:** KhÃ¡m phÃ¡ vÃ  phÃ¢n tÃ­ch quÃ¡ trÃ¬nh Hierarchical Parsing (YÃªu cáº§u 2.1)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Ná»™i dung Notebook\n",
    "\n",
    "1. [Setup & Import](#1-setup--import)\n",
    "2. [Data Overview](#2-data-overview)\n",
    "3. [Multi-file Gathering Analysis](#3-multi-file-gathering-analysis)\n",
    "4. [Hierarchy Structure Exploration](#4-hierarchy-structure-exploration)\n",
    "5. [Reference Extraction Analysis](#5-reference-extraction-analysis)\n",
    "6. [Deduplication Statistics](#6-deduplication-statistics)\n",
    "7. [Summary & Conclusions](#7-summary--conclusions)\n",
    "\n",
    "---\n",
    "\n",
    "## YÃªu cáº§u tá»« text2.txt (Section 2.1)\n",
    "\n",
    "| YÃªu cáº§u | MÃ´ táº£ |\n",
    "|---------|-------|\n",
    "| 2.1.1 | Multi-file Gathering: Xá»­ lÃ½ `\\input`, `\\include` |\n",
    "| 2.1.2 | Hierarchy Construction: Document â†’ Sections â†’ Leaf nodes |\n",
    "| 2.1.3 | Standardization & Deduplication |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c20c4eb",
   "metadata": {},
   "source": [
    "## 1. Setup & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a62c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1.1 Import thÆ° viá»‡n\n",
    "# ============================================\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Increase recursion limit (cáº§n thiáº¿t cho LaTeX parsing phá»©c táº¡p)\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "# Disable pyparsing packrat Ä‘á»ƒ trÃ¡nh RecursionError\n",
    "try:\n",
    "    import pyparsing\n",
    "    pyparsing.ParserElement.disablePackrat()\n",
    "except (ImportError, AttributeError):\n",
    "    pass\n",
    "\n",
    "# Data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup paths\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "SRC_DIR = PROJECT_ROOT / 'src'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'output'\n",
    "\n",
    "# Add src to path Ä‘á»ƒ import modules\n",
    "sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "# Import project modules\n",
    "from parser.file_gatherer import FileGatherer\n",
    "from parser.latex_cleaner import LaTeXCleaner\n",
    "from parser.hierarchy_builder import HierarchyBuilder\n",
    "from parser.reference_extractor import ReferenceExtractor\n",
    "from parser.deduplicator import Deduplicator\n",
    "from utils.file_io import read_tex_file, save_json, load_json\n",
    "\n",
    "print(f\"âœ… Project root: {PROJECT_ROOT}\")\n",
    "print(f\"âœ… Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"âœ… Number of publications: {len(list(OUTPUT_DIR.iterdir())) if OUTPUT_DIR.exists() else 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff43fdf",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "\n",
    "Kiá»ƒm tra tá»•ng quan dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c parse tá»« cÃ¡c publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2.1 Thá»‘ng kÃª tá»•ng quan vá» publications\n",
    "# ============================================\n",
    "\n",
    "def get_publication_stats():\n",
    "    \"\"\"Láº¥y thá»‘ng kÃª tá»« táº¥t cáº£ publications Ä‘Ã£ parse\"\"\"\n",
    "    stats = []\n",
    "    \n",
    "    for pub_dir in OUTPUT_DIR.iterdir():\n",
    "        if not pub_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        pub_id = pub_dir.name\n",
    "        \n",
    "        # Check hierarchy.json\n",
    "        hierarchy_path = pub_dir / 'hierarchy.json'\n",
    "        refs_path = pub_dir / 'refs.bib'\n",
    "        metadata_path = pub_dir / 'metadata.json'\n",
    "        references_path = pub_dir / 'references.json'\n",
    "        \n",
    "        stat = {\n",
    "            'publication_id': pub_id,\n",
    "            'has_hierarchy': hierarchy_path.exists(),\n",
    "            'has_refs': refs_path.exists(),\n",
    "            'has_metadata': metadata_path.exists(),\n",
    "            'has_references': references_path.exists(),\n",
    "            'num_elements': 0,\n",
    "            'num_versions': 0,\n",
    "            'num_bibtex_entries': 0,\n",
    "            'num_candidate_refs': 0\n",
    "        }\n",
    "        \n",
    "        # Count elements\n",
    "        if hierarchy_path.exists():\n",
    "            try:\n",
    "                with open(hierarchy_path, 'r', encoding='utf-8') as f:\n",
    "                    hierarchy = json.load(f)\n",
    "                stat['num_elements'] = len(hierarchy.get('elements', {}))\n",
    "                stat['num_versions'] = len(hierarchy.get('hierarchy', {}))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Count BibTeX entries\n",
    "        if refs_path.exists():\n",
    "            try:\n",
    "                with open(refs_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                stat['num_bibtex_entries'] = len(re.findall(r'@\\w+\\{', content))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Count candidate references\n",
    "        if references_path.exists():\n",
    "            try:\n",
    "                with open(references_path, 'r', encoding='utf-8') as f:\n",
    "                    refs = json.load(f)\n",
    "                stat['num_candidate_refs'] = len(refs)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        stats.append(stat)\n",
    "    \n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "# Get statistics\n",
    "df_stats = get_publication_stats()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š Tá»”NG QUAN Dá»® LIá»†U\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Tá»•ng sá»‘ publications: {len(df_stats)}\")\n",
    "print(f\"Publications cÃ³ hierarchy.json: {df_stats['has_hierarchy'].sum()}\")\n",
    "print(f\"Publications cÃ³ refs.bib: {df_stats['has_refs'].sum()}\")\n",
    "print(f\"Publications cÃ³ metadata.json: {df_stats['has_metadata'].sum()}\")\n",
    "print(f\"Publications cÃ³ references.json: {df_stats['has_references'].sum()}\")\n",
    "print()\n",
    "print(\"ğŸ“ˆ Thá»‘ng kÃª elements:\")\n",
    "print(f\"   - Tá»•ng sá»‘ elements: {df_stats['num_elements'].sum():,}\")\n",
    "print(f\"   - Trung bÃ¬nh/publication: {df_stats['num_elements'].mean():.1f}\")\n",
    "print(f\"   - Max: {df_stats['num_elements'].max()}\")\n",
    "print(f\"   - Min (>0): {df_stats[df_stats['num_elements'] > 0]['num_elements'].min()}\")\n",
    "print()\n",
    "print(\"ğŸ“š Thá»‘ng kÃª BibTeX entries:\")\n",
    "print(f\"   - Tá»•ng sá»‘ entries: {df_stats['num_bibtex_entries'].sum():,}\")\n",
    "print(f\"   - Trung bÃ¬nh/publication: {df_stats['num_bibtex_entries'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc67cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2.2 Visualize distribution\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Elements distribution\n",
    "ax1 = axes[0, 0]\n",
    "elements_data = df_stats[df_stats['num_elements'] > 0]['num_elements']\n",
    "ax1.hist(elements_data, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax1.axvline(elements_data.mean(), color='red', linestyle='--', label=f'Mean: {elements_data.mean():.1f}')\n",
    "ax1.set_xlabel('Number of Elements')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Hierarchy Elements per Publication')\n",
    "ax1.legend()\n",
    "\n",
    "# BibTeX entries distribution\n",
    "ax2 = axes[0, 1]\n",
    "bibtex_data = df_stats[df_stats['num_bibtex_entries'] > 0]['num_bibtex_entries']\n",
    "ax2.hist(bibtex_data, bins=30, edgecolor='black', alpha=0.7, color='forestgreen')\n",
    "ax2.axvline(bibtex_data.mean(), color='red', linestyle='--', label=f'Mean: {bibtex_data.mean():.1f}')\n",
    "ax2.set_xlabel('Number of BibTeX Entries')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of BibTeX Entries per Publication')\n",
    "ax2.legend()\n",
    "\n",
    "# Versions distribution\n",
    "ax3 = axes[1, 0]\n",
    "versions_data = df_stats[df_stats['num_versions'] > 0]['num_versions']\n",
    "version_counts = versions_data.value_counts().sort_index()\n",
    "ax3.bar(version_counts.index, version_counts.values, edgecolor='black', alpha=0.7, color='coral')\n",
    "ax3.set_xlabel('Number of Versions')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Distribution of Versions per Publication')\n",
    "\n",
    "# Completeness pie chart\n",
    "ax4 = axes[1, 1]\n",
    "complete = (df_stats['has_hierarchy'] & df_stats['has_refs'] & \n",
    "            df_stats['has_metadata'] & df_stats['has_references']).sum()\n",
    "incomplete = len(df_stats) - complete\n",
    "ax4.pie([complete, incomplete], \n",
    "        labels=['Complete', 'Incomplete'], \n",
    "        autopct='%1.1f%%',\n",
    "        colors=['#2ecc71', '#e74c3c'],\n",
    "        explode=(0.05, 0))\n",
    "ax4.set_title('Publication Completeness')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_ROOT / 'notebooks' / 'fig_01_data_overview.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Figure saved: fig_01_data_overview.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c8714",
   "metadata": {},
   "source": [
    "## 3. Multi-file Gathering Analysis\n",
    "\n",
    "**YÃªu cáº§u 2.1.1:** Xá»­ lÃ½ `\\input{}`, `\\include{}` Ä‘á»ƒ gather táº¥t cáº£ files Ä‘Æ°á»£c include.\n",
    "\n",
    "### PhÆ°Æ¡ng phÃ¡p triá»ƒn khai:\n",
    "1. TÃ¬m main file (chá»©a `\\documentclass` vÃ  `\\begin{document}`)\n",
    "2. Äá»‡ quy resolve táº¥t cáº£ `\\input`, `\\include`, `\\subfile`\n",
    "3. Loáº¡i bá» files khÃ´ng Ä‘Æ°á»£c reference tá»« main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.1 Demo FileGatherer trÃªn 1 publication\n",
    "# ============================================\n",
    "\n",
    "# Chá»n 1 publication cÃ³ nhiá»u versions\n",
    "sample_pubs = [p for p in OUTPUT_DIR.iterdir() if p.is_dir()][:5]\n",
    "print(\"ğŸ“ Sample publications:\", [p.name for p in sample_pubs])\n",
    "\n",
    "# Find publication with tex folder\n",
    "for sample_pub in sample_pubs:\n",
    "    tex_dir = sample_pub / 'tex'\n",
    "    if not tex_dir.exists():\n",
    "        # Try output folder structure\n",
    "        tex_dir = Path(PROJECT_ROOT.parent / 'NMKHDL' / 'data' / '23120067' / sample_pub.name / 'tex')\n",
    "    \n",
    "    if tex_dir.exists():\n",
    "        print(f\"\\nâœ… Found tex directory: {tex_dir}\")\n",
    "        \n",
    "        # Initialize FileGatherer\n",
    "        gatherer = FileGatherer(tex_dir)\n",
    "        \n",
    "        # Get version files\n",
    "        version_files = gatherer.get_all_version_files()\n",
    "        \n",
    "        print(f\"\\nğŸ“Š File Gathering Results for {sample_pub.name}:\")\n",
    "        print(f\"   - Number of versions: {len(version_files)}\")\n",
    "        \n",
    "        for version, files in version_files.items():\n",
    "            print(f\"   - Version {version}: {len(files)} files\")\n",
    "            for f in files[:3]:  # Show first 3\n",
    "                print(f\"      â€¢ {f.name}\")\n",
    "            if len(files) > 3:\n",
    "                print(f\"      ... and {len(files) - 3} more\")\n",
    "        break\n",
    "else:\n",
    "    print(\"âš ï¸ No tex directories found. Analyzing from output data instead.\")\n",
    "    \n",
    "    # Analyze from hierarchy.json\n",
    "    multi_version_pubs = df_stats[df_stats['num_versions'] > 1]\n",
    "    print(f\"\\nPublications vá»›i nhiá»u versions: {len(multi_version_pubs)}\")\n",
    "    print(multi_version_pubs[['publication_id', 'num_versions', 'num_elements']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6940c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.2 PhÃ¢n tÃ­ch main file detection logic\n",
    "# ============================================\n",
    "\n",
    "# Show logic tá»« file_gatherer.py\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ” MAIN FILE DETECTION HEURISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Thá»© tá»± Æ°u tiÃªn Ä‘á»ƒ tÃ¬m main LaTeX file:\n",
    "\n",
    "1. FILE NAMES PHá»” BIáº¾N:\n",
    "   - main.tex\n",
    "   - paper.tex\n",
    "   - article.tex\n",
    "   - manuscript.tex\n",
    "\n",
    "2. CONTENT MARKERS:\n",
    "   - Pháº£i chá»©a \\\\documentclass\n",
    "   - VÃ€ \\\\begin{document}\n",
    "   \n",
    "3. FALLBACK - Most includes:\n",
    "   - Äáº¿m sá»‘ lÆ°á»£ng \\\\input/\\\\include\n",
    "   - Chá»n file cÃ³ nhiá»u nháº¥t\n",
    "\n",
    "CODE IMPLEMENTATION:\n",
    "\"\"\")\n",
    "\n",
    "# Show code snippet\n",
    "code_snippet = '''\n",
    "def find_main_file(self, version_dir) -> Optional[Path]:\n",
    "    # Strategy 1: Common names\n",
    "    for name in ['main.tex', 'paper.tex', 'article.tex']:\n",
    "        candidate = version_dir / name\n",
    "        if candidate.exists() and self._is_main_file(candidate):\n",
    "            return candidate\n",
    "    \n",
    "    # Strategy 2: Has documentclass AND begin{document}\n",
    "    for tex_file in version_dir.glob('*.tex'):\n",
    "        content = read_tex_file(tex_file)\n",
    "        if '\\\\\\\\documentclass' in content and '\\\\\\\\begin{document}' in content:\n",
    "            return tex_file\n",
    "    \n",
    "    # Strategy 3: Most includes\n",
    "    candidates = [(f, count_includes(f)) for f in tex_files]\n",
    "    return max(candidates, key=lambda x: x[1])[0]\n",
    "'''\n",
    "print(code_snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dcbfc6",
   "metadata": {},
   "source": [
    "## 4. Hierarchy Structure Exploration\n",
    "\n",
    "**YÃªu cáº§u 2.1.2:** XÃ¢y dá»±ng cáº¥u trÃºc phÃ¢n cáº¥p:\n",
    "- Document â†’ Sections â†’ Subsections â†’ Paragraphs â†’ Leaf nodes\n",
    "- Leaf nodes: Sentences, Block Formulas, Figures/Tables\n",
    "- Itemize/Enumerate: Higher component vá»›i má»—i item lÃ  next-level element\n",
    "- **Exclude:** References, Bibliography\n",
    "- **Include:** Acknowledgements, Appendices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082814ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4.1 Analyze hierarchy.json structure\n",
    "# ============================================\n",
    "\n",
    "def analyze_hierarchy(hierarchy_path):\n",
    "    \"\"\"PhÃ¢n tÃ­ch chi tiáº¿t cáº¥u trÃºc hierarchy\"\"\"\n",
    "    with open(hierarchy_path, 'r', encoding='utf-8') as f:\n",
    "        hierarchy = json.load(f)\n",
    "    \n",
    "    elements = hierarchy.get('elements', {})\n",
    "    versions = hierarchy.get('hierarchy', {})\n",
    "    \n",
    "    # Count element types\n",
    "    element_types = Counter()\n",
    "    for elem_id in elements.keys():\n",
    "        # Extract type from ID (e.g., \"2504-13946-sec-1\" -> \"sec\")\n",
    "        parts = elem_id.split('-')\n",
    "        if len(parts) >= 3:\n",
    "            elem_type = parts[2] if len(parts) > 3 else parts[-1]\n",
    "            # Clean up numeric suffixes\n",
    "            elem_type = re.sub(r'\\d+$', '', elem_type)\n",
    "            element_types[elem_type] += 1\n",
    "    \n",
    "    return {\n",
    "        'num_elements': len(elements),\n",
    "        'num_versions': len(versions),\n",
    "        'element_types': dict(element_types),\n",
    "        'sample_elements': list(elements.items())[:5]\n",
    "    }\n",
    "\n",
    "# Analyze all publications\n",
    "hierarchy_analyses = []\n",
    "for pub_dir in OUTPUT_DIR.iterdir():\n",
    "    hierarchy_path = pub_dir / 'hierarchy.json'\n",
    "    if hierarchy_path.exists():\n",
    "        try:\n",
    "            analysis = analyze_hierarchy(hierarchy_path)\n",
    "            analysis['pub_id'] = pub_dir.name\n",
    "            hierarchy_analyses.append(analysis)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š HIERARCHY STRUCTURE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Aggregate element types\n",
    "all_element_types = Counter()\n",
    "for analysis in hierarchy_analyses:\n",
    "    all_element_types.update(analysis['element_types'])\n",
    "\n",
    "print(\"\\nğŸ“Œ Element Types Across All Publications:\")\n",
    "for elem_type, count in all_element_types.most_common(15):\n",
    "    print(f\"   {elem_type:15s}: {count:,}\")\n",
    "\n",
    "# Show sample hierarchy\n",
    "if hierarchy_analyses:\n",
    "    sample = hierarchy_analyses[0]\n",
    "    print(f\"\\nğŸ“„ Sample Hierarchy ({sample['pub_id']}):\")\n",
    "    print(f\"   Elements: {sample['num_elements']}\")\n",
    "    print(f\"   Versions: {sample['num_versions']}\")\n",
    "    print(f\"\\n   Sample elements:\")\n",
    "    for elem_id, content in sample['sample_elements']:\n",
    "        content_preview = str(content)[:60] + \"...\" if len(str(content)) > 60 else str(content)\n",
    "        print(f\"   - {elem_id}: {content_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b85b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4.2 Visualize element type distribution\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of element types\n",
    "ax1 = axes[0]\n",
    "top_types = all_element_types.most_common(10)\n",
    "types, counts = zip(*top_types) if top_types else ([], [])\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.8, len(types)))\n",
    "ax1.barh(list(types)[::-1], list(counts)[::-1], color=colors[::-1], edgecolor='black')\n",
    "ax1.set_xlabel('Count')\n",
    "ax1.set_title('Top 10 Element Types in Hierarchy')\n",
    "for i, (t, c) in enumerate(zip(types[::-1], counts[::-1])):\n",
    "    ax1.text(c + max(counts)*0.02, i, f'{c:,}', va='center', fontsize=9)\n",
    "\n",
    "# Pie chart of high-level vs leaf elements\n",
    "ax2 = axes[1]\n",
    "high_level = ['doc', 'sec', 'subsec', 'subsubsec', 'chap']\n",
    "leaf_types = ['sent', 'eq', 'formula', 'fig', 'item', 'tab']\n",
    "\n",
    "high_count = sum(all_element_types.get(t, 0) for t in high_level)\n",
    "leaf_count = sum(all_element_types.get(t, 0) for t in leaf_types)\n",
    "other_count = sum(all_element_types.values()) - high_count - leaf_count\n",
    "\n",
    "sizes = [high_count, leaf_count, other_count]\n",
    "labels = [f'High-level\\n({high_count:,})', f'Leaf nodes\\n({leaf_count:,})', f'Other\\n({other_count:,})']\n",
    "colors_pie = ['#3498db', '#2ecc71', '#95a5a6']\n",
    "ax2.pie(sizes, labels=labels, colors=colors_pie, autopct='%1.1f%%', \n",
    "        startangle=90, explode=(0.02, 0.02, 0.02))\n",
    "ax2.set_title('Element Classification')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_ROOT / 'notebooks' / 'fig_02_hierarchy_elements.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Figure saved: fig_02_hierarchy_elements.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bc35ed",
   "metadata": {},
   "source": [
    "## 5. Reference Extraction Analysis\n",
    "\n",
    "**YÃªu cáº§u 2.1.3:** TrÃ­ch xuáº¥t references tá»«:\n",
    "1. File `.bib` (BibTeX files)\n",
    "2. File `.bbl` (compiled bibliography)\n",
    "3. `\\bibitem` trong `.tex` files â†’ Convert sang BibTeX chuáº©n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f5051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5.1 Analyze refs.bib files\n",
    "# ============================================\n",
    "\n",
    "def analyze_bibtex(bib_path):\n",
    "    \"\"\"PhÃ¢n tÃ­ch file refs.bib\"\"\"\n",
    "    with open(bib_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Count entry types\n",
    "    entry_types = re.findall(r'@(\\w+)\\{', content)\n",
    "    type_counts = Counter(entry_types)\n",
    "    \n",
    "    # Extract fields\n",
    "    titles = re.findall(r'title\\s*=\\s*\\{([^}]+)\\}', content, re.IGNORECASE)\n",
    "    authors = re.findall(r'author\\s*=\\s*\\{([^}]+)\\}', content, re.IGNORECASE)\n",
    "    years = re.findall(r'year\\s*=\\s*\\{?(\\d{4})\\}?', content, re.IGNORECASE)\n",
    "    dois = re.findall(r'doi\\s*=\\s*\\{([^}]+)\\}', content, re.IGNORECASE)\n",
    "    \n",
    "    return {\n",
    "        'num_entries': len(entry_types),\n",
    "        'entry_types': dict(type_counts),\n",
    "        'has_title': len(titles),\n",
    "        'has_author': len(authors),\n",
    "        'has_year': len(years),\n",
    "        'has_doi': len(dois)\n",
    "    }\n",
    "\n",
    "# Analyze all refs.bib files\n",
    "bibtex_analyses = []\n",
    "for pub_dir in OUTPUT_DIR.iterdir():\n",
    "    bib_path = pub_dir / 'refs.bib'\n",
    "    if bib_path.exists():\n",
    "        try:\n",
    "            analysis = analyze_bibtex(bib_path)\n",
    "            analysis['pub_id'] = pub_dir.name\n",
    "            bibtex_analyses.append(analysis)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Aggregate statistics\n",
    "df_bibtex = pd.DataFrame(bibtex_analyses)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“š REFERENCE EXTRACTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPublications with refs.bib: {len(df_bibtex)}\")\n",
    "print(f\"Total BibTeX entries: {df_bibtex['num_entries'].sum():,}\")\n",
    "print(f\"Average entries per publication: {df_bibtex['num_entries'].mean():.1f}\")\n",
    "\n",
    "# Entry type distribution\n",
    "all_entry_types = Counter()\n",
    "for analysis in bibtex_analyses:\n",
    "    all_entry_types.update(analysis['entry_types'])\n",
    "\n",
    "print(\"\\nğŸ“Œ Entry Types Distribution:\")\n",
    "for entry_type, count in all_entry_types.most_common(10):\n",
    "    print(f\"   @{entry_type:15s}: {count:,}\")\n",
    "\n",
    "# Field completeness\n",
    "print(\"\\nğŸ“Š Field Completeness:\")\n",
    "print(f\"   Entries with title:  {df_bibtex['has_title'].sum():,} ({df_bibtex['has_title'].sum()/df_bibtex['num_entries'].sum()*100:.1f}%)\")\n",
    "print(f\"   Entries with author: {df_bibtex['has_author'].sum():,} ({df_bibtex['has_author'].sum()/df_bibtex['num_entries'].sum()*100:.1f}%)\")\n",
    "print(f\"   Entries with year:   {df_bibtex['has_year'].sum():,} ({df_bibtex['has_year'].sum()/df_bibtex['num_entries'].sum()*100:.1f}%)\")\n",
    "print(f\"   Entries with DOI:    {df_bibtex['has_doi'].sum():,} ({df_bibtex['has_doi'].sum()/df_bibtex['num_entries'].sum()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f69257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5.2 Visualize BibTeX extraction results\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Entry types pie chart\n",
    "ax1 = axes[0]\n",
    "top_entry_types = all_entry_types.most_common(6)\n",
    "types, counts = zip(*top_entry_types) if top_entry_types else ([], [])\n",
    "colors_entry = plt.cm.Set3(np.linspace(0, 1, len(types)))\n",
    "ax1.pie(counts, labels=[f'@{t}\\n({c:,})' for t, c in top_entry_types], \n",
    "        colors=colors_entry, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('BibTeX Entry Type Distribution')\n",
    "\n",
    "# Field completeness bar chart\n",
    "ax2 = axes[1]\n",
    "total_entries = df_bibtex['num_entries'].sum()\n",
    "fields = ['Title', 'Author', 'Year', 'DOI']\n",
    "completeness = [\n",
    "    df_bibtex['has_title'].sum() / total_entries * 100,\n",
    "    df_bibtex['has_author'].sum() / total_entries * 100,\n",
    "    df_bibtex['has_year'].sum() / total_entries * 100,\n",
    "    df_bibtex['has_doi'].sum() / total_entries * 100\n",
    "]\n",
    "colors_field = ['#2ecc71' if c > 70 else '#f39c12' if c > 40 else '#e74c3c' for c in completeness]\n",
    "bars = ax2.bar(fields, completeness, color=colors_field, edgecolor='black')\n",
    "ax2.set_ylabel('Completeness (%)')\n",
    "ax2.set_title('Field Completeness in BibTeX Entries')\n",
    "ax2.set_ylim(0, 100)\n",
    "for bar, pct in zip(bars, completeness):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "             f'{pct:.1f}%', ha='center', fontsize=10)\n",
    "\n",
    "# Entries per publication histogram\n",
    "ax3 = axes[2]\n",
    "ax3.hist(df_bibtex['num_entries'], bins=25, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax3.axvline(df_bibtex['num_entries'].mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {df_bibtex[\"num_entries\"].mean():.1f}')\n",
    "ax3.set_xlabel('Number of BibTeX Entries')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('BibTeX Entries per Publication')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_ROOT / 'notebooks' / 'fig_03_bibtex_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Figure saved: fig_03_bibtex_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99722cb5",
   "metadata": {},
   "source": [
    "## 6. Deduplication Statistics\n",
    "\n",
    "**YÃªu cáº§u 2.1.3:** Deduplication á»Ÿ 2 levels:\n",
    "1. **Reference Deduplication:** Loáº¡i bá» duplicate entries (title similarity > 95% AND author overlap > 80%)\n",
    "2. **Content Deduplication:** Full-text match giá»¯a cÃ¡c versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071805d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6.1 Analyze deduplication (from hierarchy.json)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”„ DEDUPLICATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Deduplication logic explanation\n",
    "print(\"\"\"\n",
    "ğŸ“Œ Reference Deduplication Strategy:\n",
    "   1. Identify duplicates: title_similarity > 95% AND author_overlap > 80%\n",
    "   2. Select canonical key (entry with most complete fields)\n",
    "   3. Unionize fields from all duplicates\n",
    "   4. Rename \\\\cite{old_key} â†’ \\\\cite{canonical_key}\n",
    "\n",
    "ğŸ“Œ Content Deduplication Strategy:\n",
    "   - Full-text match sau khi cleanup\n",
    "   - Elements giá»‘ng nhau giá»¯a versions â†’ share same ID\n",
    "   - Reduce redundancy trong hierarchy.json\n",
    "\"\"\")\n",
    "\n",
    "# Analyze content sharing across versions\n",
    "content_sharing_stats = []\n",
    "\n",
    "for pub_dir in OUTPUT_DIR.iterdir():\n",
    "    hierarchy_path = pub_dir / 'hierarchy.json'\n",
    "    if hierarchy_path.exists():\n",
    "        try:\n",
    "            with open(hierarchy_path, 'r', encoding='utf-8') as f:\n",
    "                hierarchy = json.load(f)\n",
    "            \n",
    "            elements = hierarchy.get('elements', {})\n",
    "            versions = hierarchy.get('hierarchy', {})\n",
    "            \n",
    "            if len(versions) > 1:\n",
    "                # Check for shared elements across versions\n",
    "                all_element_ids = set()\n",
    "                version_elements = {}\n",
    "                \n",
    "                for v, h in versions.items():\n",
    "                    v_elements = set(h.keys())\n",
    "                    version_elements[v] = v_elements\n",
    "                    all_element_ids.update(v_elements)\n",
    "                \n",
    "                # Find shared elements\n",
    "                if len(version_elements) >= 2:\n",
    "                    v_list = list(version_elements.values())\n",
    "                    shared = v_list[0].intersection(*v_list[1:])\n",
    "                    \n",
    "                    content_sharing_stats.append({\n",
    "                        'pub_id': pub_dir.name,\n",
    "                        'num_versions': len(versions),\n",
    "                        'total_unique_elements': len(all_element_ids),\n",
    "                        'shared_elements': len(shared),\n",
    "                        'sharing_ratio': len(shared) / len(all_element_ids) if all_element_ids else 0\n",
    "                    })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if content_sharing_stats:\n",
    "    df_sharing = pd.DataFrame(content_sharing_stats)\n",
    "    print(f\"\\nğŸ“Š Content Sharing Analysis (Publications with >1 version):\")\n",
    "    print(f\"   Publications analyzed: {len(df_sharing)}\")\n",
    "    print(f\"   Average shared elements: {df_sharing['shared_elements'].mean():.1f}\")\n",
    "    print(f\"   Average sharing ratio: {df_sharing['sharing_ratio'].mean()*100:.1f}%\")\n",
    "    print(f\"   Max sharing ratio: {df_sharing['sharing_ratio'].max()*100:.1f}%\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No multi-version publications found for content sharing analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319dcdbf",
   "metadata": {},
   "source": [
    "## 7. Summary & Conclusions\n",
    "\n",
    "### ğŸ“Š Tá»•ng káº¿t Parsing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7.1 Final Summary Statistics\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“‹ PARSING PIPELINE SUMMARY (Requirement 2.1 Compliance)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Total Publications',\n",
    "        'Publications with hierarchy.json',\n",
    "        'Publications with refs.bib',\n",
    "        'Total Hierarchy Elements',\n",
    "        'Total BibTeX Entries',\n",
    "        'Average Elements/Publication',\n",
    "        'Average BibTeX/Publication',\n",
    "        'Multi-version Publications',\n",
    "        'Parse Success Rate'\n",
    "    ],\n",
    "    'Value': [\n",
    "        len(df_stats),\n",
    "        df_stats['has_hierarchy'].sum(),\n",
    "        df_stats['has_refs'].sum(),\n",
    "        df_stats['num_elements'].sum(),\n",
    "        df_stats['num_bibtex_entries'].sum(),\n",
    "        f\"{df_stats['num_elements'].mean():.1f}\",\n",
    "        f\"{df_stats['num_bibtex_entries'].mean():.1f}\",\n",
    "        df_stats[df_stats['num_versions'] > 1].shape[0],\n",
    "        f\"{df_stats['has_hierarchy'].mean()*100:.1f}%\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… REQUIREMENT COMPLIANCE CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "compliance = {\n",
    "    '2.1.1 Multi-file Gathering': 'âœ… Implemented (FileGatherer class)',\n",
    "    '2.1.2 Hierarchy Construction': 'âœ… Implemented (HierarchyBuilder class)',\n",
    "    '2.1.3a LaTeX Cleanup': 'âœ… Implemented (LaTeXCleaner class)',\n",
    "    '2.1.3b Math Normalization': 'âœ… Implemented (inline â†’ $...$, block â†’ equation)',\n",
    "    '2.1.3c Reference Extraction': 'âœ… Implemented (ReferenceExtractor class)',\n",
    "    '2.1.3d Deduplication': 'âœ… Implemented (Deduplicator class)'\n",
    "}\n",
    "\n",
    "for req, status in compliance.items():\n",
    "    print(f\"   {req}: {status}\")\n",
    "\n",
    "print(\"\\nğŸ“ Output Files Generated:\")\n",
    "print(\"   â€¢ hierarchy.json - Hierarchical structure\")\n",
    "print(\"   â€¢ refs.bib - Deduplicated BibTeX entries\")\n",
    "print(\"   â€¢ metadata.json - Paper metadata\")\n",
    "print(\"   â€¢ references.json - Candidate references\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
